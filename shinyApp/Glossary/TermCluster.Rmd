---
title: "Term-Definition Clustering"
author: "Kyle"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## Introduction

The Synonym and Related tables in the [**IoW Glossary**](http://purl.org/iow/Glossary) are populated by members of subgraphs
of Term nodes connected by edges of SKOS properties `skos:exactMatch` and `skos:related`, respectively.
In order to facilitate the process of adding `skos:related` and `skos:exactMatch` properties to 
concepts in [**vocbench3**](http://purl.org/iow/vocbench3), we cluster terms by the text content of their definitions.

This will allow us to identify which terms are semantically related, or semantically synonymous from automatically generated of terms that have similar textual content, rather than manually, exhaustively searching over all terms in the glossary. This process is sure to generate plenty of false negatives, but it is a good start.

The following vignette describes and implements a simple way to programmtically group the terms in the glossary to help humans encode these simple semantic relationships into the IoW Glossary. A lot of it is adapted from the `textmineR` vignette.  

## Clustering Documents

There are several ways to cluster "documents". A classic, simple way from computational linguistics
is to treat each document as an object with a "bag of words". This bag of words can be vectorized, such that a collection of documents
can be represented as a table, where each row represents a document and each column a word occuring in any of the documents, and each cell representing the count of that word in each document. From this tabular setup, classical clustering algortihms such as `K-means` or `agglomerative hierarchical` can be used to group documents together. Below we apply text mining tools in R to implement this idea.

### Vectorizing documents

For us, a "document" is the string representation of the definition of a term , identified by its IoW Glossary URI (universal resource identifier). We need to read in our data, and transform it into a document term matrix (DTM). Here, we load the necessary packages, and read in our glossary.

```{r}
library(textmineR)
library(tidyverse)
library(rdflib)
library(RCurl)
library(tm)

g<-rdf_parse("glossary.ttl",format="turtle")
q<-'PREFIX g: <http://purl.org/iow/Glossary#>
PREFIX grddl: <http://www.w3.org/2003/g/data-view#>
PREFIX dct: <http://purl.org/dc/terms/>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX skosxl: <http://www.w3.org/2008/05/skos-xl#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX fn: <http://www.w3.org/2005/xpath-functions#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
PREFIX sesame: <http://www.openrdf.org/schema/sesame#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX dc: <http://purl.org/dc/elements/1.1/>

SELECT ?uri ?Term ?Vocabulary ?voclink ?Organization ?Definition ?Theme
WHERE {
   ?uri a g:Terms .
    ?uri skosxl:prefLabel ?l .
    ?l   skosxl:literalForm ?Term .

    ?uri skos:inScheme ?v .
    ?v skosxl:prefLabel ?vl .
    ?vl skosxl:literalForm ?Vocabulary .

    ?v skos:note ?voclink .
    ?v skos:scopeNote ?o .
    ?o skosxl:altLabel ?ol .
    ?ol skosxl:literalForm ?Organization .
    ?uri skos:definition ?Definition .

  OPTIONAL{ ?t a g:Themes .
   ?uri skos:broader ?t .
         ?t skosxl:prefLabel ?tl .
        ?tl skosxl:literalForm ?Theme .}


}'

data<-rdf_query(g,q)
data<-distinct(data,uri,.keep_all=T)
```

Now, we create the DTM:

```{r}
dtm <- CreateDtm(doc_vec = data$Definition, # character vector of documents
                 doc_names = data$uri, # document names
                 ngram_window = c(1, 2), # minimum and maximum n-gram length
                 stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
                                  stopwords::stopwords(source = "smart"),"water"), # this is the default value
                 lower = TRUE, # lowercase - this is the default value
                 remove_punctuation = TRUE, # punctuation - this is the default
                 remove_numbers = TRUE, # numbers - this is the default
                 verbose = FALSE) # default is all available cpus on the system

# construct the matrix of term counts 
tf_mat <- TermDocFreq(dtm)
```


As with any clustering algorithm, it is important to scale the cells to filter out the noise generated by factors that are common across many items. We can do this with term [**frequency-inverse document frequency**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)


```{r}
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
```

### Calculate document distance matrix


Then we calculate the cosine dissimilarity, a common distance metric for clustering word frequencies.

```{r}
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
cd<-na.omit(cdist)
```

### Cluster

```{r}
 hclust_dist<- cdist
  hclust_dist[is.na(hclust_dist)] <- 1
  hclust_dist[is.nan(hclust_dist)] <- 1
  sum(is.infinite(hclust_dist)) 

hc <- hclust(hclust_dist, "ward.D2")

clustering <- cutree(hc, 50)

plot(hc, main = "Hierarchical clustering of glossary terms",
     ylab = "", xlab = "", yaxt = "n")

rect.hclust(hc, 50, border = "red")

data$clust50tf<-clustering
```
```{r}
p_words <- colSums(dtm) / sum(dtm)

cluster_words <- lapply(unique(clustering), function(x){
  rows <- dtm[ clustering == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})

cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)
cluster_summary
```


## Happy Knitting!

Feel free to use the `knitr` infrastructure with dozens of tunable options in
your package vignette.

```{r fig.width=6, fig.height=6, fig.align='center'}
set.seed(123)
n <- 1000
x1  <- matrix(rnorm(n), ncol = 2)
x2  <- matrix(rnorm(n, mean = 3, sd = 1.5), ncol = 2)
x   <- rbind(x1, x2)
head(x)
smoothScatter(x, xlab = "x1", ylab = "x2")
```

You can also include code snippets of languages other than R, but note that
the block header has no curly brackets around the language name.

```cpp
// [[Rcpp::export]]
NumericVector timesTwo(NumericVector x) {
    return x * 2;
}
```

You can write math expressions, e.g. $Y = X\beta + \epsilon$,
footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(iris, 10))
```

## Stay Tuned

Please visit the [development page](http://github.com/yixuan/prettydoc/) of the 
`prettydoc` package for latest updates and news. Comments, bug reports and
pull requests are always welcome.

